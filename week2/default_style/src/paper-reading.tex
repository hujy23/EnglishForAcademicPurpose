\documentclass{article}
\usepackage{graphicx}
\author{Hu Jiyuan}
\title{Paper Reading Report}
\begin{document}
\maketitle

\section{Requirements}

\paragraph{} Draw an outline for the paper and summarize the main idea of each part.
\paragraph{} Quote sentences typical of a part that you can use in your own paper.
\paragraph{} Quote sentences you find difficult to understand.
\paragraph{} Record the bibliographic information of the paper 
(1) by following its documentation style and 
(2) in the Chicago style.

\section{Report}

\subsection{Outline \& main idea}

\includegraphics[width=\textwidth]{graphics/part1-overview.png}  
\paragraph{ABSTRACT} answers the following four questions: 
    \par What problems have people met?
    \par What's your solution?
    \par How does the solution work?
    \par How well does the solution work in comparison of existing solutions?

\paragraph{Part 1: INTRODUCTION} first introduces the background and need of the 
neural networks compression solutions: reduce the storage and energy required 
to run inference on large networks. Then briefly describes their solutions in three
parts: pruning, trained quantization and Huffman coding.

\paragraph{Part 2: NETWORK PRUNING} shows how to prune the network. Neural networks 
have many connections, they prune the small-weight connections.

\paragraph{Part 3: TRAINED QUANTIZATION AND WEIGHT SHARING} continues to compress 
the network by reducing the number of bits required to represent each weight, which 
is what called quantization. They use k-means clustering to find the weight that 
can be shared. 

\paragraph{Part 4: HUFFMAN CODING} is used in the purpose of encoding (or compression) 
the quantized weights and sparse matrix index generated by the former step.

\paragraph{Part 5: EXPERIMENTS} are taken to show the performance of their solutions by 
comparing the network parameters and accuracy of several networks before and after 
their solutions are taken.

\paragraph{Part 6: DISCUSSIONS} discuss the relationship between performance and 
configuration.

\paragraph{Part 7: RELATED WORK} is the extension of the background description in 
Part 1: introduction. It mainly tells us the neural networks are over-parametrized 
and introduce the typical methods taken to solve the problem by other people. 

\paragraph{Part 8: FUTURE WORK} denotes what they are going to do next: build software 
to complete the benchmarked test they does not finish because of the existing software 
does not support their solution. What's more, to build hardware (ASIC chip) to achieve 
their solutions.

\paragraph{Part 9: CONCLUSION} concludes their steps, key performance and application 
scene of their solutions.

\paragraph{REFERENCES} are in alphabetical order.

\subsection{Typical sentences}

\paragraph{To address} this limitation, we introduce “something”, some introduction.

\paragraph{Examples:} To
address this limitation, we introduce “deep compression”, a three stage pipeline:
pruning, trained quantization and Huffman coding, that work together to reduce
the storage requirement of neural networks by 35× to 49× without affecting their
accuracy.

To achieve this goal, we present “deep compression”: a three stage pipeline (Figure 1) 
to reduce the storage required by neural network in a manner that preserves the 
original accuracy.

\paragraph{Figure} xxx is illustrated in Figure x.
\paragraph{Examples:} Weight sharing is illustrated in Figure 3.

\subsection{Difficult sentences}

\paragraph{} While the \emph{pruned} network has been benchmarked on various hardware, 
the \emph{quantized} network with weight sharing has not, because off-the-shelf 
cuSPARSE or MKL SPBLAS library does not support indirect matrix entry lookup, nor is 
the relative index in CSC or CSR format supported. So the full
advantage of Deep Compression that fit the model in cache is not fully unveiled. 

\subsection{Bibliographic information}

\paragraph{} Han, Song, Mao, Huizi and Dally, William J. Deep compression: compressing 
deep neural networks with pruning, trained quantization and huffman coding. In \emph{
International Conference on Learning Representations}, 2016

\paragraph{} Han, Song, Mao, Huizi and Dally, William J. ``Deep compression: compressing 
deep neural networks with pruning, trained quantization and huffman coding.'' 
International Conference on Learning Representations (2016). https://arxiv.org/abs/1510.00149

\paragraph{Google scholar:} Han, Song, Huizi Mao, and William J. Dally. ``Deep compression: Compressing deep 
neural networks with pruning, trained quantization and huffman coding.'' \emph{arXiv 
preprint arXiv:1510.00149} (2015).

\end{document}
